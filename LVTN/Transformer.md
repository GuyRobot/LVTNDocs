The Transformer was proposed in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762)The Transformer models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder then generates an output  sequence of symbols one element at a time. At each step the model is auto-regressive consuming the previously generated symbols as additional input when generating the next.
The Transformer using stacked self-attention and point-wise, fully  
connected layers for both the encoder and decoder, shown in below.

https://builtin.com/artificial-intelligence/transformer-neural-network
https://www.jeremyjordan.me/transformer-architecture/
https://arxiv.org/pdf/1706.03762.pdf
http://jalammar.github.io/illustrated-transformer/
https://www.tensorflow.org/text/tutorials/transformer