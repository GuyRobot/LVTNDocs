The Transformer was proposed in the paper [Attention is All You Need](https://arxiv.org/abs/1706.03762)The Transformer models have an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder then generates an output  sequence of symbols one element at a time. At each step the model is auto-regressive consuming the previously generated symbols as additional input when generating the next.
The Transformer using stacked self-attention and point-wise, fully  
connected layers for both the encoder and decoder, shown in below.